{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3imfPunwFgif"
   },
   "source": [
    "# ENPH 353 LogBook\n",
    "\n",
    "Name: Yousif El-Wishahy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OHCKrKUFgih"
   },
   "source": [
    "## Lab 1 - Jan 13th 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0A_h8fIFgii"
   },
   "source": [
    "Worked on installing lubuntu on usb flash drive, got the .iso file and flashed it to usb with balenaetcher. One problem was that it ran pretty slow - will have to consider this for next time. Other than that, I watched a video about basic linux terminal commands, ran the gazebo simulation and noted the real-time factor (im assuming this related to performance). Linux was slow but otherwise worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBARit9jFgij"
   },
   "source": [
    "## Lab 2 - Jan 20 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkN0KwZFFgik"
   },
   "source": [
    "### Attempting daul boot and vm for faster lubuntu speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2l3iqKlFgik"
   },
   "source": [
    "The usb linux installing is too slow, I'm going to try to daul boot it on a hard drive partition on my laptop instead.\n",
    "\n",
    "Update: After 2 hours of troubleshooting , I could not manage to get things to work. I got to the stage of clearing a partition for the linux installing on  the drive. But when I booted from the usb i flashed the linux .iso to, there are no installing options as almost every online source says there should be. Initially, I could not even boot to the grub menu and had disabled fast boot in the bios. \n",
    "\n",
    "Update 2: I decided to go with a vm on my desktop instead. I installed all the required files and downloaded the iso and ran the conversion command - note that i had to modify the command for directory and file name: \n",
    "```\n",
    "VBoxManage convertfromraw ~/Downloads/lubuntu_18.04_21-01-18_UEFI.iso lubunt_ENPH353_v2.vmdk --format vmdk\n",
    "```\n",
    "\n",
    "From there things went well until I created the vm from the converted .iso file. I enabled the uefi setting in the vm and increased ram allocation, but when the vm booted it only went into the uefi shell. After some online research, turns out i had to modify the startup.nsh file to include '\\EFI\\ubuntu\\grubx64.efi ' and then things worked. From there the lubuntu vm worked and was connected to the web. Notably, it ran MUCH better in vm than off usb but that might be attributed to the 24gb ram or 12 core cpu I have on my desktop :) - I suspect the main reason though, is data transfer speeds of usb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnndWAgOFgil"
   },
   "source": [
    "### Actual lab 2: programming line detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpkxpGgbFgin"
   },
   "source": [
    "This lab consisted of two learning goals for me:\n",
    "1. Understanding how to utilize OpenCV\n",
    "2. Implementing line following using OpenCV and numpy\n",
    "\n",
    "I've worked with python, jupyter and numpy a lot during the past year or two in personal projects, courses and my internship where I process radar signals using low pass filters and such.\n",
    "\n",
    "Anyways, here's what I am try to achieve:\n",
    "**There is a video containing a traversal of a path, we want to make a ball follow this path using numpy and OpenCV**\n",
    "\n",
    "Breaking it down:\n",
    "* Split the video into frames and process each frame\n",
    "* Draw a circle onto frame to show 'ball' following path, no need for 3d animations\n",
    "* Detect path to place circle roughly at the centre of the path\n",
    "* This should make it look like the ball is following the path\n",
    "\n",
    "OpenCV does the heavy lifting for converting the video into an array of frames for me to process, from there I was able to use numpy to look for the location of all pixels containing the colour of the path. To get the cartesian image coordinates of where the ball should be, I just averages all the x coordinates and y coordinates of the points where the path color appeared. Then use OpenCV to draw the circle at that averaged point.\n",
    "\n",
    "This algorithm worked well, and the task was complete. \n",
    "\n",
    "Considerations for further improvements:\n",
    "* the algorithm doesn't have to search the whole frame for the path if we know its usually closer to the bottom for example\n",
    "* the color to detect was manually inputted, is there a way to automate this?\n",
    "* could edge detection be a better method instead of averaging color coordinates? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDKhbKxaNqbq"
   },
   "source": [
    "### Lab 2 results demo\n",
    "\n",
    "https://drive.google.com/file/d/1XesD-TIiiiouQgJ0daAhKpeUp1_MtAX-/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBUqpoRfFgio"
   },
   "source": [
    "## Lab 3 Jan 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvEIpjoxOWPU"
   },
   "source": [
    "### Dual booting ubuntu on laptop instead\n",
    "\n",
    "The vm was still too slow, so i dual booted ubuntu as i couldn't get the lubuntu .iso to install on a partition. So I dual booted latest ubuntu and installled all required packages and it ran much smoother for the gazebo simulation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A5koBrlOtbp"
   },
   "source": [
    "### Self driving robot with computer vision and PID in gazebo/ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cf3MEcGFgip"
   },
   "source": [
    "**Goal:** Simulate a robot in a ROS and gazebo environment and make the robot drive and follow a path by implementing computer vision and control software.\n",
    "\n",
    "**Tools/skills applied and learned:**\n",
    "* Linux environment \n",
    "* ROS (robot operating system)\n",
    "* Gazebo (Simulation world) \n",
    "* Python\n",
    "* OpenCV and computer vision\n",
    "* Control theory (PID?)\n",
    "\n",
    "The first portion of the lab consisted of building and running the gazebo simulation world and robot. The robot and world setup in ros uses an xml style code where components of the world or robot are added in xml with features and textures. \n",
    "\n",
    "Next external plugins are supported that move the robot and prove camera feeds. \n",
    "\n",
    "I installed these plugins:\n",
    "1. skid steer controller\n",
    "2. camera\n",
    "\n",
    "The skid steer controller essentially controls the robot to turn as a skid steer vehicle would, with forward driving and skid steering. The control works by publishing to the robot through command line or python script. The publishing works something like this:\n",
    "\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#imports\n",
    "import rospy\n",
    "from geometry_msgs.msg import Twist\n",
    "\n",
    "#register publisher to ros simulation \n",
    "rospy.init_node('robot_driver')\n",
    "drive_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=1)\n",
    "\n",
    "#create and send drive commands\n",
    "move = Twist()\n",
    "move.angular.z = 5\n",
    "move.linear.x = 0.25\n",
    "drive_pub.publish(move)    \n",
    "\n",
    "```\n",
    "\n",
    "With this we are able to control the robot through python script! Obviously this code will be run in a loop with updates!\n",
    "\n",
    "But how do we know where the robot is and where the path is relative to it so we can steer it? Well, we **place a camera on the robot and stream its feed to the controller**. Camera support is added through the gazebo camera sensor plugin. All we need to do is setup a subscriber to receive the camera feed and do something useful with it!\n",
    "\n",
    "The control loop is as follows then:\n",
    "1. Receive camera subscriber feed from robot\n",
    "2. process image in useful data\n",
    "3. use data to generate move commands\n",
    "4. publish move commands to robot\n",
    "5. repeat\n",
    "\n",
    "Registering a subscriber to the camera feed is done like this:\n",
    "```\n",
    "image_sub = rospy.Subscriber(\"/robot/camera1/image_raw\",Image ,self.callback,queue_size=3)\n",
    "```\n",
    "\n",
    "There is a useful librrary called CV_bride that converts this camera feed into images OpenCV can work with; we want to use OpenCV for the camera feed processing!\n",
    "\n",
    "```\n",
    "#in constructor\n",
    "self.bridge = CvBridge()\n",
    "\n",
    "#setup callback function for subscriber to call every time there is a camera feed input\n",
    "def callback(self, img):\n",
    "    try:\n",
    "        self.latest_img = self.bridge.imgmsg_to_cv2(img,\"bgr8\")   \n",
    "        self.empty = False\n",
    "     except CvBridgeError as e:\n",
    "         print(e)              \n",
    "```\n",
    "\n",
    "The camera feed class is now completed!\n",
    "```\n",
    "class image_converter:\n",
    "    def __init__(self):\n",
    "        self.bridge = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber(\"/robot/camera1/image_raw\",Image ,self.callback,queue_size=3)\n",
    "        self.latest_img = Image()\n",
    "        self.empty = True\n",
    "    def callback(self, img):\n",
    "        try:\n",
    "            self.latest_img = self.bridge.imgmsg_to_cv2(img,\"bgr8\")   \n",
    "            self.empty = False\n",
    "        except CvBridgeError as e:\n",
    "            print(e)        \n",
    "```\n",
    "\n",
    "Every time the robot camera sends over video frames to the subscriber, this class converts and stores the latest image!\n",
    "\n",
    "Now we need to process this image. In lab two we worked on image processing. We will convert image colours and detect the path colour and then find the path position relative to the robot centre and call that 'displacement'\n",
    "\n",
    "This displacement can then be scaled into useful error values in a certain range!\n",
    "\n",
    "```\n",
    "            image = self.camera.latest_img\n",
    "\n",
    "            #image/dimension bounds\n",
    "            xcenter = image.shape[1]/2\n",
    "            max_reading_error = image.shape[1]/2\n",
    "            min_reading_error = 25\n",
    "            h_threshold = image.shape[0] - 200\n",
    "\n",
    "            #convert colour\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            image_gray = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # cv2.imshow(\"Image window\", image_gray)\n",
    "            # cv2.waitKey(1)\n",
    "\n",
    "            gray_clr = [128,128,128]\n",
    "\n",
    "            #locate path pixels based on colour\n",
    "            Y,X = np.where(np.all(image_gray==gray_clr,axis=2))\n",
    "            if X.size != 0 and Y.size != 0:\n",
    "                self.move_state = 0\n",
    "                displacement = xcenter - int(np.average(X))\n",
    "                sign = displacement/np.abs(displacement) \n",
    "\n",
    "                # xerror = map(np.abs(displacement), min_reading_error, \n",
    "                # max_reading_error, 0, max)\n",
    "                xerror = sign*np.interp(np.abs(displacement),\n",
    "                [min_reading_error,max_reading_error],\n",
    "                [0,max_error])\n",
    "```\n",
    "\n",
    "We could simply generate move commands based on this scaled error, however this is very unstable and results in over steering. \n",
    "\n",
    "The better approach is to use PID control (proportionality - integration - derivative). This is an important concept in control theory and is very applicable to many control situations. I will try to explain it to the best of my knowledge. Essentially the code above has given us an error value representing how far the robot is off the path. We could simply tell the robot to steer to the left if the error indicates it is to the right, but by how much? \n",
    "\n",
    "Now consider that the control loop I mentioned previously with 5 steps has a time delay between receieving the data from the camera and outputting movement data to the robot. In a real world or multi threaded simulation, the robot is still moving while the loop is trying to calculate how much it should steer. This delay in feedback causes the robot to oversteer or understeer and we end up seeing it either oscillate wildy on the path or stray completely off the path. \n",
    "\n",
    "Ever recall watching professional racecar driver do a quick turn of the steering wheel before the vechile even reacts? Well they know about the delay and they turn accordingly before they receive feedback. \n",
    "\n",
    "Back to PID control : we're trying to account for the feedback delay and avoid osccillation. Proportionality represents prodocing feedback control proportional to the error receive ~ a large error should have a proportionally large steering command. \n",
    "\n",
    "Integration accounts for past values of the error by integrating the error over time. This helps prevent reisidual error in control by taking into account historic error values.\n",
    "\n",
    "Differentiation is taking the derivative of the error over time, usually the time step is based on the interval between each control loop. This helps predict future error trends. \n",
    "\n",
    "If you implement steering proportional to error, you have control that's unstable. If you add the derivative term you are able to predict the error trend so as to not over steer or under steer.\n",
    "\n",
    "The code looks like this with scaling parameters Kd and Kp for tuning.\n",
    "```\n",
    "    def calculate_pid(self,error):\n",
    "        curr_time = rospy.get_time()\n",
    "        time_step = curr_time - self.last_time\n",
    "        error_step = error - self.last_error\n",
    "\n",
    "        if time_step == 0:\n",
    "            time_step = 1\n",
    "        #inst. derivative\n",
    "        derivative = error_step/time_step\n",
    "\n",
    "        d=K_D*derivative\n",
    "        p=K_P*error\n",
    "\n",
    "        return d+p\n",
    "\n",
    "    def get_move_cmd(self):\n",
    "        error = self.get_error()\n",
    "        move = Twist()\n",
    "    \n",
    "        if(self.move_state == 0):\n",
    "            g = self.calculate_pid(error)\n",
    "\n",
    "            move.angular.z = g * multiplier\n",
    "            move.linear.x = np.interp(move.angular.z,\n",
    "                    [0,max_error],\n",
    "                    [0.5,0])\n",
    "        elif self.move_state == 1:\n",
    "            move.angular.z = 2.5\n",
    "        elif self.move_state == 2:\n",
    "            move.linear.x = -0.25                \n",
    "\n",
    "        print('z',move.angular.z)    \n",
    "        print('x',move.linear.x) \n",
    "\n",
    "        return move    \n",
    "```\n",
    "\n",
    "\n",
    "A common tuning method of the kp and kd parameters is to set kd to zero and increase kp until the robot begins to oscillate around the path. Then reduce kp by about half and start increasing kd to diminish oscillations (aka dont over or under steer).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3bT9qsGNjqE"
   },
   "source": [
    "### Lab 3 results demo\n",
    "https://docs.google.com/spreadsheets/d/1jZXvnHNauXIl3l8w_PlOWoTgnQO2wzeh-mBbFFjNER4/edit#gid=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8uoQxzAOF7h"
   },
   "source": [
    "## Lab 4 - Feb 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaNvSa_ZOOKM"
   },
   "source": [
    "### GUI and tracking using sift\n",
    "\n",
    "Skills used:\n",
    "* QT and python gui creation\n",
    "* SIFT tracking of realtime video feed\n",
    "\n",
    "Below I will test some feature tracking and perspective transformation algorithms based on SIFT and matrix math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy4MYj9LoGQz",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img/kerbal.png\" ,cv2.IMREAD_GRAYSCALE)  # queryiamge\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame',frame)\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"img/kerbal-space.jpg\" ,cv2.IMREAD_GRAYSCALE)  # queryiamge\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "# Features\n",
    "sift = cv2.SIFT_create()\n",
    "kp_image, desc_image = sift.detectAndCompute(img, None)\n",
    "# Feature matching\n",
    "index_params = dict(algorithm=0, trees=5)\n",
    "search_params = dict()\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "_, frame = cap.read()\n",
    "grayframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # trainimage\n",
    "kp_grayframe, desc_grayframe = sift.detectAndCompute(grayframe, None)\n",
    "matches = flann.knnMatch(desc_image, desc_grayframe, k=2)\n",
    "good_points = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.6 * n.distance:\n",
    "        good_points.append(m)\n",
    "\n",
    "a = frame\n",
    "cv2.drawMatchesKnn(img, kp_image, grayframe, kp_grayframe, matches, a)\n",
    "cv2.imshow('Matches',a)\n",
    "cv2.waitKey(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv2.imshow('a',a)\n",
    "cv2.waitKey(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"img/kerbal-space.jpg\" ,cv2.IMREAD_GRAYSCALE)  # queryiamge\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "# Features\n",
    "sift = cv2.SIFT_create()\n",
    "kp_image, desc_image = sift.detectAndCompute(img, None)\n",
    "# Feature matching\n",
    "index_params = dict(algorithm=0, trees=5)\n",
    "search_params = dict()\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "_, frame = cap.read()\n",
    "grayframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # trainimage\n",
    "kp_grayframe, desc_grayframe = sift.detectAndCompute(grayframe, None)\n",
    "matches = flann.knnMatch(desc_image, desc_grayframe, k=2)\n",
    "good_points = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.6 * n.distance:\n",
    "        good_points.append(m)\n",
    "\n",
    "query_pts = np.float32([kp_image[m.queryIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "train_pts = np.float32([kp_grayframe[m.trainIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "matches_mask = mask.ravel().tolist()\n",
    "\n",
    "h, w = img.shape\n",
    "pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
    "dst = cv2.perspectiveTransform(pts, matrix)\n",
    "\n",
    "homography = cv2.polylines(frame, [np.int32(dst)], True, (255, 0, 0), 3)\n",
    "cv2.imshow(\"Homography\", homography)\n",
    "cv2.waitKey(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def track_img(source_img, video_frame):\n",
    "    # Features\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp_image, desc_image = sift.detectAndCompute(source_img, None)\n",
    "    # Feature matching\n",
    "    index_params = dict(algorithm=0, trees=5)\n",
    "    search_params = dict()\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    grayframe = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)  # trainimage\n",
    "    kp_grayframe, desc_grayframe = sift.detectAndCompute(grayframe, None)\n",
    "    matches = flann.knnMatch(desc_image, desc_grayframe, k=2)\n",
    "    good_points = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.6 * n.distance:\n",
    "            good_points.append(m)\n",
    "\n",
    "    query_pts = np.float32([kp_image[m.queryIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "    train_pts = np.float32([kp_grayframe[m.trainIdx].pt for m in good_points]).reshape(-1, 1, 2)\n",
    "    matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "\n",
    "    h, w = source_img.shape\n",
    "    pts = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
    "    dst = cv2.perspectiveTransform(pts, matrix)\n",
    "\n",
    "    homography = cv2.polylines(frame, [np.int32(dst)], True, (255, 0, 0), 3)\n",
    "\n",
    "    return homography\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"img/kerbal-space.jpg\" ,cv2.IMREAD_GRAYSCALE)  # queryiamge\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame',track_img(img,frame))\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ENPH353-LogBook-Yousif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}